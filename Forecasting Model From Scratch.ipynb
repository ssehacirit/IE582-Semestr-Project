{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Preprocessing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "MAIN_SOURCE = True\n",
    "\n",
    "LIVE_PRED_START = pd.Timestamp.today() + pd.to_timedelta(1, \"d\")\n",
    "LIVE_PRED_START = LIVE_PRED_START.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "LIVE_PRED_END = pd.to_datetime(LIVE_PRED_START) + pd.to_timedelta(1, \"d\")\n",
    "LIVE_PRED_END = LIVE_PRED_END.date().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "START_HOUR = 9\n",
    "END_HOUR = 18\n",
    "\n",
    "print(\"Pred Date Start: \", LIVE_PRED_START, \"Pred Date End: \", LIVE_PRED_END)\n",
    "\n",
    "stock_price_files = glob(\"../Dataset/*\")\n",
    "\n",
    "stock_price_list = [pd.read_csv(v) for v in stock_price_files]\n",
    "stock_price_df = pd.concat(stock_price_list, ignore_index=True)\n",
    "stock_price_df = stock_price_df.drop_duplicates().reset_index(drop=True)\n",
    "stock_price_df[\"timestamp\"] = pd.to_datetime(\n",
    "    stock_price_df.timestamp).dt.tz_localize(None)\n",
    "stock_price_df.sort_values(\n",
    "    by=[\"short_name\", \"timestamp\"], ignore_index=True, inplace=True\n",
    ")\n",
    "stock_price_df.drop_duplicates(ignore_index=True, inplace=True)\n",
    "stock_price_df[\"date\"] = stock_price_df.timestamp.dt.date\n",
    "stock_price_df[\"hour\"] = stock_price_df.timestamp.dt.hour\n",
    "\n",
    "# Yahoo Features\n",
    "\n",
    "START_DATE = \"2018-01-02\"\n",
    "END_DATE = LIVE_PRED_END\n",
    "\n",
    "def pull_yahoo_features(stock_name, start_date, end_date, action = True):\n",
    "    yf_api = yf.Ticker(stock_name)\n",
    "    stock_data = yf_api.history(\n",
    "        start=start_date, end=end_date, interval=\"1d\", actions=action\n",
    "        ).reset_index()\n",
    "    if action:\n",
    "        stock_data.columns = [\n",
    "            \"date\", \"OPEN\", \"HIGH\",\n",
    "            \"LOW\", \"CLOSE\", \"VOLUME\", \"DIVIDENDS\", \"STOCKSPLITS\"]\n",
    "    else:\n",
    "        stock_data.columns = [\n",
    "            \"date\", \"OPEN\", \"HIGH\", \"LOW\", \"CLOSE\", \"VOLUME\"]\n",
    "        \n",
    "    if \".IS\" in stock_name:\n",
    "        stock_data[\"date\"] = pd.to_datetime(\n",
    "            stock_data.date).dt.tz_localize(None)\n",
    "    else:\n",
    "        stock_data[\"date\"] = pd.to_datetime(\n",
    "            stock_data.date).dt.tz_convert(\n",
    "                \"Europe/Istanbul\").dt.tz_localize(None)\n",
    "        stock_data.drop(columns=[\"VOLUME\"], inplace=True)\n",
    "        \n",
    "    stock_data[\"date\"] = stock_data.date.dt.date\n",
    "    stock_data = stock_data.melt(id_vars=[\"date\"])\n",
    "    stock_data[\"short_name\"] = stock_name.split(\".\")[0]\n",
    "    \n",
    "    return stock_data\n",
    "\n",
    "stock_name_yahoo = [f\"{v}.IS\" for v in stock_price_df.short_name.unique()] + [\"XU030.IS\"]\n",
    "\n",
    "# bist features\n",
    "\n",
    "bist_stocks = [\n",
    "    pull_yahoo_features(v, START_DATE, END_DATE) for v in stock_name_yahoo\n",
    "    ]\n",
    "bist_stocks = pd.concat(bist_stocks, ignore_index=True)\n",
    "\n",
    "drop_indices = bist_stocks[\n",
    "    (bist_stocks.short_name == \"XU030\") & \n",
    "    (bist_stocks.variable.isin([\"DIVIDENDS\", \"STOCKSPLITS\"]))\n",
    "].index.to_list()\n",
    "\n",
    "bist_stocks = bist_stocks[~bist_stocks.index.isin(drop_indices)].reset_index(drop=True)\n",
    "\n",
    "# global features\n",
    "\n",
    "global_stocks_name = [\"CL=F\", \"USDTRY=X\", \"EURTRY=X\", \"GC=F\"]\n",
    "\n",
    "global_stocks = [\n",
    "    pull_yahoo_features(v, START_DATE, END_DATE, action=False) for v in global_stocks_name\n",
    "    ]\n",
    "global_stocks = pd.concat(global_stocks, ignore_index=True)\n",
    "\n",
    "# global markets\n",
    "\n",
    "global_market_name = [\"^N225\", \"^NY\", \"^NDX\"]\n",
    "\n",
    "global_markets = [\n",
    "    pull_yahoo_features(v, START_DATE, END_DATE, action=False) for v in global_market_name\n",
    "]\n",
    "global_markets = pd.concat(global_markets, ignore_index=True)\n",
    "\n",
    "all_ohlc_features = pd.concat([\n",
    "    bist_stocks, global_stocks, global_markets\n",
    "], ignore_index=True)\n",
    "\n",
    "# TR related features\n",
    "\n",
    "inflation_tr = pd.read_csv(\"/home/ssc/Desktop/Untitled 1.csv\")\n",
    "inflation_tr.columns = [\"date\", \"YEARLYCHNG\", \"MONTHLYCHNG\"]\n",
    "inflation_tr[\"date\"] = pd.to_datetime(inflation_tr.date).dt.date\n",
    "inflation_tr[[\"YEARLYCHNG\", \"MONTHLYCHNG\"]] = inflation_tr[[\"YEARLYCHNG\", \"MONTHLYCHNG\"]] / 100\n",
    "inflation_tr = inflation_tr.melt(id_vars=[\"date\"])\n",
    "inflation_tr[\"short_name\"] = \"INFRATE\"\n",
    "\n",
    "overall_external_features = pd.concat(\n",
    "    [all_ohlc_features, inflation_tr], ignore_index=True)\n",
    "overall_external_features[\"variable\"] = (\n",
    "    overall_external_features[\"short_name\"] + \"_\" + overall_external_features[\"variable\"]\n",
    "    )\n",
    "\n",
    "org_stock_names = list(stock_price_df.short_name.unique())\n",
    "\n",
    "stock_price_pivot = stock_price_df.pivot_table(\n",
    "    index=[\"timestamp\", \"date\", \"hour\"], columns=\"short_name\", values=\"price\"\n",
    ").reset_index().rename_axis(None, axis=1)\n",
    "stock_price_pivot[org_stock_names] = stock_price_pivot[org_stock_names].interpolate()\n",
    "\n",
    "live_pred_empty_data = pd.date_range(\n",
    "    start=LIVE_PRED_START, end=LIVE_PRED_END, freq=\"h\", closed=\"left\"\n",
    ")\n",
    "live_pred_empty_df = pd.DataFrame(live_pred_empty_data, columns=[\"timestamp\"])\n",
    "live_pred_empty_df[\"date\"] = live_pred_empty_df.timestamp.dt.date\n",
    "live_pred_empty_df[\"hour\"] = live_pred_empty_df.timestamp.dt.hour\n",
    "live_pred_empty_df = live_pred_empty_df[(live_pred_empty_df[\"hour\"] >= START_HOUR) & (live_pred_empty_df[\"hour\"] <= END_HOUR)].reset_index(drop=True)\n",
    "\n",
    "stock_price_pivot = stock_price_pivot.merge(live_pred_empty_df, on=[\"date\", \"hour\", \"timestamp\"], how=\"outer\")\n",
    "\n",
    "overall_external_features_pivot = overall_external_features.pivot_table(\n",
    "    index=[\"date\"], columns=\"variable\", values=\"value\"\n",
    ").reset_index().rename_axis(None, axis=1)\n",
    "overall_external_features_pivot[\"INFRATE_MONTHLYCHNG\"] = overall_external_features_pivot[\"INFRATE_MONTHLYCHNG\"].ffill() \n",
    "overall_external_features_pivot[\"INFRATE_YEARLYCHNG\"] = overall_external_features_pivot[\"INFRATE_YEARLYCHNG\"].ffill()\n",
    "\n",
    "overall_external_features_pivot = overall_external_features_pivot.interpolate()\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "ROLL_DAYS = [5, 20, 60, 120] \n",
    "rolling_iterations = product(org_stock_names, ROLL_DAYS)\n",
    "\n",
    "rolling_df = stock_price_pivot[[\"timestamp\", \"date\", \"hour\"]].copy()\n",
    "for stock, day in rolling_iterations:\n",
    "    rolling_df[f\"ROLLING_{day}DAY_MEAN_{stock}\"] = stock_price_pivot[stock].rolling(day * 10, closed=\"left\").mean()\n",
    "    rolling_df[f\"ROLLING_{day}DAY_STD_{stock}\"] = stock_price_pivot[stock].rolling(day * 10, closed=\"left\").std()\n",
    "    rolling_df[f\"ROLLING_{day}DAY_MEDIAN_{stock}\"] = stock_price_pivot[stock].rolling(day * 10, closed=\"left\").median()\n",
    "    rolling_df[f\"ROLLING_{day}DAY_MIN_{stock}\"] = stock_price_pivot[stock].rolling(day * 10, closed=\"left\").min()\n",
    "    rolling_df[f\"ROLLING_{day}DAY_MAX_{stock}\"] = stock_price_pivot[stock].rolling(day * 10, closed=\"left\").max()\n",
    "\n",
    "LAG_DAYS = [1, 2, 3, 4, 5] \n",
    "lag_iterations = product(org_stock_names, LAG_DAYS)\n",
    "\n",
    "lag_df = stock_price_pivot[[\"timestamp\", \"date\", \"hour\"]].copy()\n",
    "for stock, day in lag_iterations:\n",
    "    lag_df[f\"LAG_{day}DAY_{stock}\"] = stock_price_pivot[stock].shift(10 * day)\n",
    "\n",
    "ar_features = rolling_df.merge(lag_df, how=\"left\")\n",
    "\n",
    "overall_rolling_vols = []\n",
    "for day in ROLL_DAYS:\n",
    "    volume_roll_df = overall_external_features_pivot.filter(like=\"VOLUME\").rolling(day).mean()\n",
    "    volume_roll_df.columns = [f\"ROLLING_{day}DAY_MEAN_{x}\" for x in list(volume_roll_df)]\n",
    "    overall_rolling_vols.append(volume_roll_df)\n",
    "overall_rolling_vols_df = pd.concat(overall_rolling_vols, axis=1)\n",
    "\n",
    "overall_external_features_pivot = pd.concat(\n",
    "    [overall_external_features_pivot, overall_rolling_vols_df], axis=1)\n",
    "\n",
    "# Target\n",
    "import numpy as np\n",
    "\n",
    "response_df = stock_price_pivot.copy()\n",
    "    \n",
    "response_df[org_stock_names] = response_df[org_stock_names].pct_change(periods=1, limit=1).shift(-1)\n",
    "\n",
    "drop_cols = [\"timestamp\", \"date\"] + org_stock_names\n",
    "\n",
    "model_df = response_df.merge(overall_external_features_pivot, how=\"left\", on=[\"date\"]).merge(ar_features, how=\"left\")\n",
    "model_df[\"date\"] = pd.to_datetime(model_df[\"date\"])\n",
    "\n",
    "model_df[\"year\"] = model_df.date.dt.year\n",
    "model_df[\"month\"] = model_df.date.dt.month\n",
    "model_df[\"day\"] = model_df.date.dt.day\n",
    "model_df[\"dow\"] = model_df.date.dt.dayofweek\n",
    "model_df[\"quarter\"] = model_df.date.dt.quarter\n",
    "model_df[\"doy\"] = model_df.date.dt.dayofyear\n",
    "model_df[\"woy\"] = model_df.date.dt.isocalendar().week.astype(\"int\")\n",
    "\n",
    "model_df[\"is_monday_morning\"] = 0\n",
    "model_df.loc[(model_df.dow == 0) & (model_df.hour < 13), \"is_monday_morning\"] = 1\n",
    "model_df[\"is_friday_noon\"] = 0\n",
    "model_df.loc[(model_df.dow == 4) & (model_df.hour >= 13), \"is_friday_noon\"] = 1\n",
    "\n",
    "model_df[\"date\"] = model_df[\"date\"].astype(\"str\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "LIVE_PRED_START = pd.Timestamp.today() + pd.to_timedelta(1, \"d\")\n",
    "LIVE_PRED_START = LIVE_PRED_START.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "LIVE_PRED_END = pd.to_datetime(LIVE_PRED_START) + pd.to_timedelta(1, \"d\")\n",
    "LIVE_PRED_END = LIVE_PRED_END.date().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "latest_actual_date = \"2024-01-11\"\n",
    "\n",
    "START_HOUR = 9\n",
    "END_HOUR = 18\n",
    "\n",
    "stock_price_files = glob(\"../Dataset/*\")\n",
    "\n",
    "stock_price_list = [pd.read_csv(v) for v in stock_price_files]\n",
    "stock_price_df = pd.concat(stock_price_list, ignore_index=True)\n",
    "stock_price_df = stock_price_df.drop_duplicates().reset_index(drop=True)\n",
    "stock_price_df[\"timestamp\"] = pd.to_datetime(\n",
    "    stock_price_df.timestamp).dt.tz_localize(None)\n",
    "stock_price_df.sort_values(\n",
    "    by=[\"short_name\", \"timestamp\"], ignore_index=True, inplace=True\n",
    ")\n",
    "stock_price_df.drop_duplicates(ignore_index=True, inplace=True)\n",
    "stock_price_df[\"date\"] = stock_price_df.timestamp.dt.date\n",
    "stock_price_df[\"hour\"] = stock_price_df.timestamp.dt.hour\n",
    "\n",
    "stock_names = list(stock_price_df.short_name.unique())\n",
    "\n",
    "processed = model_df.copy()\n",
    "predictors = processed.drop(columns=stock_names)\n",
    "\n",
    "target_raw = stock_price_df.pivot_table(\n",
    "    index=[\"timestamp\", \"date\", \"hour\"], columns=\"short_name\", values=\"price\"\n",
    ").reset_index().rename_axis(None, axis=1)\n",
    "target_raw[stock_names] = target_raw[stock_names].interpolate()\n",
    "\n",
    "# Target TL Diff\n",
    "# Bir sonraki saat fiyat X TL arttı veya azaldı\n",
    "\n",
    "live_pred_empty_data = pd.date_range(\n",
    "    start=LIVE_PRED_START, end=LIVE_PRED_END, freq=\"h\", closed=\"left\"\n",
    ")\n",
    "live_pred_empty_df = pd.DataFrame(live_pred_empty_data, columns=[\"timestamp\"])\n",
    "live_pred_empty_df[\"date\"] = live_pred_empty_df.timestamp.dt.date\n",
    "live_pred_empty_df[\"hour\"] = live_pred_empty_df.timestamp.dt.hour\n",
    "live_pred_empty_df = live_pred_empty_df[(live_pred_empty_df[\"hour\"] >= START_HOUR) & (live_pred_empty_df[\"hour\"] <= END_HOUR)].reset_index(drop=True)\n",
    "live_pred_empty_df = live_pred_empty_df[[\"date\", \"hour\"]].copy()\n",
    "\n",
    "target_try_return = target_raw[[\"date\", \"hour\"]].copy()\n",
    "for stock in stock_names:\n",
    "    target_try_return[stock] = target_raw[stock].diff(periods=1).shift(-1)\n",
    "target_try_return = target_try_return.merge(live_pred_empty_df, on=[\"date\", \"hour\"], how=\"outer\")\n",
    "\n",
    "target_log_return = target_raw[[\"date\", \"hour\"]].copy()\n",
    "for stock in stock_names:\n",
    "    log_transformed = target_raw[stock].apply(np.log)\n",
    "    target_log_return[stock] = log_transformed.diff(periods=1).shift(-1)\n",
    "target_log_return = target_log_return.merge(live_pred_empty_df, on=[\"date\", \"hour\"], how=\"outer\")\n",
    "\n",
    "target_pct_return = target_raw[[\"date\", \"hour\"]].copy()\n",
    "for stock in stock_names:\n",
    "    target_pct_return[stock] = target_raw[stock].pct_change(periods=1, limit=1).shift(-1)\n",
    "target_pct_return = target_pct_return.merge(live_pred_empty_df, on=[\"date\", \"hour\"], how=\"outer\")\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "TEST_START = LIVE_PRED_START\n",
    "\n",
    "param = {\n",
    "    'objective': 'regression',\n",
    "    'boosting_type': 'rf',\n",
    "    'lambda_l1': 8.816979942542755,\n",
    "    'lambda_l2': 3.281542231961584e-06,\n",
    "    'num_leaves': 138,\n",
    "    'feature_fraction': 0.9948710619824198,\n",
    "    'bagging_fraction': 0.43847967094014917,\n",
    "    'min_child_samples': 92,\n",
    "    'max_depth': 5,\n",
    "    'num_iterations': 114,\n",
    "    'boost_from_average': True,\n",
    "    'learning_rate': 0.04548612752166902,\n",
    "    'bagging_freq': 1\n",
    "    }\n",
    "\n",
    "def different_targets_to_model(predictors_df, target_df, test_start, params):\n",
    "\n",
    "    predictors_df[\"date\"] = pd.to_datetime(predictors_df.date)\n",
    "    target_df[\"date\"] = pd.to_datetime(target_df.date)\n",
    "    model_df = target_df.merge(predictors_df, how=\"right\")\n",
    "\n",
    "    drop_cols = [\"timestamp\", \"date\"] + stock_names\n",
    "\n",
    "    train_data = model_df[model_df.date < test_start].reset_index(drop=True).dropna(axis=0).reset_index(drop=True)\n",
    "    test_data = model_df[model_df.date >= test_start].reset_index(drop=True)\n",
    "\n",
    "    X_train = train_data.drop(columns=drop_cols)\n",
    "    y_train = train_data[stock_names].copy()\n",
    "    X_test = test_data.drop(columns=drop_cols)\n",
    "\n",
    "    model = MultiOutputRegressor(\n",
    "        lgb.LGBMRegressor(**params), \n",
    "        n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    return preds\n",
    "\n",
    "log_param = param.copy()\n",
    "log_param[\"objective\"] = \"mape\"\n",
    "\n",
    "pct_param = param.copy()\n",
    "pct_param[\"objective\"] = \"regression_l1\"\n",
    "\n",
    "pct_return_preds = different_targets_to_model(predictors.copy(), target_pct_return.copy(), TEST_START, params=pct_param)\n",
    "log_return_preds = different_targets_to_model(predictors.copy(), target_log_return.copy(), TEST_START, params=log_param)\n",
    "try_return_preds = different_targets_to_model(predictors.copy(), target_try_return.copy(), TEST_START, params=param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_predicion(raw_preds, latest_actual_date, is_pct_return = False):\n",
    "\n",
    "    close_baseline = target_raw[(target_raw.date == pd.to_datetime(latest_actual_date)) & (target_raw.hour == 18)].reset_index(drop=True)\n",
    "    close_baseline = close_baseline.melt(id_vars=[\"timestamp\", \"date\", \"hour\"]).reset_index(drop=True)\n",
    "    close_baseline.drop(columns=[\"date\", \"hour\", \"timestamp\"], inplace=True)\n",
    "\n",
    "    test_period = target_pct_return.loc[(target_pct_return.date >= pd.to_datetime(TEST_START)), [\"date\", \"hour\"]].copy()\n",
    "    test_period[stock_names] = raw_preds\n",
    "    test_period = test_period.melt(id_vars=[\"date\", \"hour\"], value_name=\"prediction_return\")\n",
    "\n",
    "    test_results = test_period.merge(close_baseline, how=\"left\")\n",
    "\n",
    "    if is_pct_return:\n",
    "        test_results[\"prediction_price\"] = test_results[\"value\"] * (1 + test_results[\"prediction_return\"])\n",
    "    else:\n",
    "        test_results[\"prediction_price\"] = test_results[\"value\"] + test_results[\"prediction_return\"]\n",
    "    test_results.rename(columns={\"variable\": \"short_name\"}, inplace=True)\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "pct_df = final_predicion(pct_return_preds, latest_actual_date, True)[[\"date\", \"hour\", \"short_name\", \"prediction_price\"]]\n",
    "log_df = final_predicion(log_return_preds, latest_actual_date, False)[[\"date\", \"hour\", \"short_name\", \"prediction_price\"]]\n",
    "try_df = final_predicion(try_return_preds, latest_actual_date, False)[[\"date\", \"hour\", \"short_name\", \"prediction_price\"]]\n",
    "\n",
    "ensemble_pred_df = pct_df[[\"date\", \"hour\", \"short_name\"]].copy()\n",
    "ensemble_pred_df[\"prediction\"] = np.mean([pct_df.prediction_price.values, log_df.prediction_price.values, try_df.prediction_price.values], axis=0)\n",
    "\n",
    "import json\n",
    "\n",
    "submission_data = ensemble_pred_df[ensemble_pred_df.date == pd.to_datetime(LIVE_PRED_START)].reset_index(drop=True)\n",
    "submission_data = submission_data.pivot_table(index=[\"date\", \"short_name\"], columns=\"hour\", values=\"prediction\").reset_index().rename_axis(None, axis=1)\n",
    "\n",
    "submission_dict = {}\n",
    "for stock in stock_names:\n",
    "    preds = submission_data.query(f\"short_name == '{stock}'\").iloc[0, 2:].to_list()\n",
    "    submission_dict[stock] = preds\n",
    "\n",
    "with open(f\"./{LIVE_PRED_START}_stock_predictions.json\", \"w\") as stock_file:\n",
    "    json.dump(submission_dict, stock_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foreflux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
